{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPETITOR ANALYSIS EDSA INTERNSHIP PROJECT\n",
    "Explore Data Science wants to develop a marketing startegy to ensure that all social media content posted reaches the inteded target market or audiance and that our social media market are growing in engagement and also analysing our growing social media engagement as compared to the competitors\n",
    "## This notebook will be used in scraping Our competitors data from LinkedIn webpage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Importing our projects libraries/dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pandas as pd\n",
    "import re as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Getting our competitors url to get the data from\n",
    "- By first getting the company page that the user wants to scrape.\n",
    "- Then by checking for an existing user credentials file or create one if its a first time user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if existing user credential file exists or create one \n",
    "try:\n",
    "    f= open(\"linkedin_credentials.txt\",\"r\")\n",
    "    contents = f.read()\n",
    "    username = contents.replace(\"=\",\",\").split(\",\")[1]\n",
    "    password = contents.replace(\"=\",\",\").split(\",\")[3]\n",
    "except:\n",
    "    f= open(\"linkedin_credentials.txt\",\"w+\")\n",
    "    username = input('Enter your linkedin username: ')\n",
    "    password = input('Enter your linkedin password: ')\n",
    "    f.write(\"username={}, password={}\".format(username,password))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Getting Selenium and ChromeDriver ready\n",
    "We now need to get Selenium to use ChromeDriver to open chrome and visit Linkedin where it will sign in using your login info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing Chromedriver\n",
    "browser = webdriver.Chrome('chromedriver')\n",
    "\n",
    "\n",
    "#Open login page\n",
    "browser.get('https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin')\n",
    "\n",
    "#Enter login info:\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys(password)\n",
    "elementID.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Defining our variables and looping through them to access our elements\n",
    "We are going to define different list that we intend to append data to and our competitors list that we want to scrape data from from and then attach the elements that we have access to to our defied lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list that we intend to append our scraped elements\n",
    "competitors_name = []\n",
    "post_dates = []\n",
    "post_texts = []\n",
    "post_likes = []\n",
    "post_comments = []\n",
    "post_shared = []\n",
    "media_type = []\n",
    "total_followers = []\n",
    "\n",
    "# Creating a list of competitors that we wish to scrape data from\n",
    "pages = [\"https://www.linkedin.com/company/explore-datascience/\", \n",
    "         \"https://www.linkedin.com/company/simplilearn/\",\n",
    "         \"https://www.linkedin.com/school/hyperion-development-south-africa/\", \n",
    "         \"https://www.linkedin.com/company/africa-data-school/\", \n",
    "         \"https://www.linkedin.com/school/moringa-school/\", \n",
    "         \"https://www.linkedin.com/company/data-science-dojo/\", \n",
    "         \"https://www.linkedin.com/school/nyc-data-science-academy/\", \n",
    "         \"https://www.linkedin.com/school/udacity/\", \n",
    "         \"https://www.linkedin.com/school/getsmarter/\"]\n",
    "\n",
    "\n",
    "# looping through our competitors lists\n",
    "for page in pages:\n",
    "    company_name = page[33:-1]\n",
    "\n",
    "    #Go to company post webpage\n",
    "    browser.get(page + 'posts/')\n",
    "    \n",
    "    company_page = browser.page_source\n",
    "\n",
    "    #Checking out our page source code\n",
    "    linkedin_soup = bs(company_page.encode(\"utf-8\"), \"html\")\n",
    "    linkedin_soup.prettify()\n",
    "    \n",
    "    # Defining our container element that contains our main elements that we wish to scrape from our web page\n",
    "    containers = linkedin_soup.findAll(\"div\",{\"class\":\"occludable-update ember-view\"})\n",
    "    \n",
    "    #Simulate scrolling to capture all posts\n",
    "    SCROLL_PAUSE_TIME = 1.5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    # Iterating through our container element within our web page and accessing our elements that we wish to have acces to\n",
    "    for container in containers:\n",
    "\n",
    "            try:\n",
    "                competitor = container.find(\"span\", {\"class\":\"feed-shared-actor__name\"})\n",
    "                posted_date = container.find(\"span\",{\"class\":\"visually-hidden\"})\n",
    "                text_box = container.find(\"div\",{\"class\":\"feed-shared-update-v2__description-wrapper\"})\n",
    "                text = text_box.find(\"span\",{\"dir\":\"ltr\"})\n",
    "                post_shares = container.findAll(\"li\", {'class':[\"social-details-social-counts__item--with-social-proof\",\"social-details-social-counts__comments social-details-social-counts__item\"]})\n",
    "                new_likes = container.findAll(\"li\", {'class':[\"social-details-social-counts__item--with-social-proof\",\"social-details-social-counts__item\"]})\n",
    "                new_comments = container.findAll(\"li\", {\"class\": \"social-details-social-counts__item social-details-social-counts__comments social-details-social-counts__item--with-social-proof\"})\n",
    "                followers = container.find(\"span\", {\"class\":\"feed-shared-actor__description\"})\n",
    "\n",
    "                competitors_name.append(competitor.text.strip())\n",
    "                post_dates.append(posted_date.text.strip())\n",
    "                post_texts.append(text.text.strip())\n",
    "                total_followers.append(followers.text.strip())\n",
    "\n",
    "\n",
    "\n",
    "                try:\n",
    "                    video_box = container.findAll(\"div\",{\"class\": \"feed-shared-update-v2__content feed-shared-linkedin-video ember-view\"})\n",
    "                    video_link = video_box[0].find(\"video\", {\"class\":\"vjs-tech\"})\n",
    "                    media_links.append(video_link['src'])\n",
    "                    media_type.append(\"Video\")\n",
    "                except:\n",
    "                    try:\n",
    "                        image_box = container.findAll(\"div\",{\"class\": \"feed-shared-image__container\"})\n",
    "                        image_link = image_box[0].find(\"img\", {\"class\":\"ivm-view-attr__img--centered feed-shared-image__image feed-shared-image__image--constrained lazy-image ember-view\"})\n",
    "                        media_links.append(image_link['src'])\n",
    "                        media_type.append(\"Image\")\n",
    "                    except:\n",
    "                        try:\n",
    "                            #mutiple shared images\n",
    "                            image_box = container.findAll(\"div\",{\"class\": \"feed-shared-image__container\"})\n",
    "                            image_link = image_box[0].find(\"img\", {\"class\":\"ivm-view-attr__img--centered feed-shared-image__image lazy-image ember-view\"})\n",
    "                            media_links.append(image_link['src'])\n",
    "                            media_type.append(\"Multiple Images\")\n",
    "                        except:\n",
    "                            try:\n",
    "                                article_box = container.findAll(\"div\",{\"class\": \"feed-shared-article__description-container\"})\n",
    "                                article_link = article_box[0].find('a', href=True)\n",
    "                                media_links.append(article_link['href'])\n",
    "                                media_type.append(\"Article\")\n",
    "                            except:\n",
    "                                try:\n",
    "                                    video_box = container.findAll(\"div\",{\"class\": \"feed-shared-external-video__meta\"})          \n",
    "                                    video_link = video_box[0].find('a', href=True)\n",
    "                                    media_links.append(video_link['href'])\n",
    "                                    media_type.append(\"Youtube Video\")   \n",
    "                                except:\n",
    "                                    try:\n",
    "                                        poll_box = container.findAll(\"div\",{\"class\": \"feed-shared-update-v2__content overflow-hidden feed-shared-poll ember-view\"})\n",
    "                                        media_links.append(\"None\")\n",
    "                                        media_type.append(\"Other: Poll, Shared Post, etc\")\n",
    "                                    except:\n",
    "                                        media_links.append(\"None\")\n",
    "                                        media_type.append(\"Unknown\")\n",
    "\n",
    "                try:\n",
    "                    post_likes.append(new_likes[0].text.strip())\n",
    "                except:\n",
    "                    post_likes.append(0)\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    post_comments.append(new_comments[0].text.strip())                           \n",
    "                except:                                                           \n",
    "                    post_comments.append(\"0\")\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    post_shared.append(post_shares[1].text.strip())                           \n",
    "                except:                                                           \n",
    "                    post_shared.append(\"0\")\n",
    "                    pass\n",
    "\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comments = container.findAll(\"li\", {\"class\": \"social-details-social-counts__item social-details-social-counts__comments social-details-social-counts__item--with-social-proof\"})\n",
    "\n",
    "print(new_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_likes = container.findAll(\"li\", {'class':[\"social-details-social-counts__item--with-social-proof\",\"social-details-social-counts__item\"]})\n",
    "print(new_likes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_likes = container.findAll(\"li\", {'class':[\"social-details-social-counts__item--with-social-proof\",\"social-details-social-counts__item\"]})\n",
    "print(new_likes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Cleaning our scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Viewing our competitors name on the platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EXPLORE Data Science Academy', 'EXPLORE Data Science Academy', 'EXPLORE Data Science Academy', 'Simplilearn', 'Simplilearn', 'Simplilearn']\n"
     ]
    }
   ],
   "source": [
    "print(competitors_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Getting the total numbers of followers for our competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(total_followers)\n",
    "followers_clean_up = []\n",
    "\n",
    "for i in total_followers:\n",
    "    c = str(i[0:-9]).replace('followers','').replace(' ', '')\n",
    "    followers_clean_up += [c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Viewing our followers count on the platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7,169', '7,169', '7,169', '340,070', '340,070', '340,070']\n"
     ]
    }
   ],
   "source": [
    "print(followers_clean_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Cleaning our date of the post column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Remove any special characters that might have been scraped during our initial process that are unnecessary in our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dates = []\n",
    "for i in post_dates:\n",
    "    d = str(i[0:3]).replace('\\n\\n', '').replace('•','').replace(' ', '')\n",
    "    cleaned_dates += [d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1w', '12', '1w', '3w', '17', '4d']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Getting our current time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-24 08:53:01.332198\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# current date and time\n",
    "now = datetime.today()\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. A logic to convert our no of days, no of weeks, no of months and years into a timestamp\n",
    "The following logic is making use of the dateutil module known as relativedelta to convert our hours, days, months and years scraped in our data into actual datetime objects in order to know the exact date and it's precise time through which a post was made by an organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "posts_posted_date = []\n",
    "\n",
    "for clean_d in cleaned_dates:\n",
    "    clean_d.split()\n",
    "    if clean_d[-1] == \"h\":\n",
    "        t_hour = int(clean_d[0])\n",
    "        posts_posted_date.append(now - relativedelta(hours = t_hour))\n",
    "    elif clean_d[-1] == \"d\":\n",
    "        t_day = datetime.timedelta(days = int(clean_d[0]))\n",
    "        posts_posted_date.append(now - t_day)\n",
    "    elif clean_d[-1] == \"w\":\n",
    "        t_week = datetime.timedelta(days = int(clean_d[0]) * 7)\n",
    "        posts_posted_date.append(now - t_week)\n",
    "    elif clean_d[-1] == \"m\":\n",
    "        t_month = int(clean_d[0])\n",
    "        posts_posted_date.append(now - relativedelta(months = t_month))\n",
    "    elif clean_d == \"y\":\n",
    "        t_year = int(clean_d[0])\n",
    "        posts_posted_date.append(now - relativedelta(years = t_year))\n",
    "    else:\n",
    "#         posts_posted_date.append(timestamp - (int(clean_d) * 60 * 1000))\n",
    "        posts_posted_date.append(now - relativedelta(minutes= 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Printing our date of the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2022, 5, 17, 8, 53, 1, 332198), datetime.datetime(2022, 5, 24, 8, 23, 1, 332198), datetime.datetime(2022, 5, 17, 8, 53, 1, 332198), datetime.datetime(2022, 5, 3, 8, 53, 1, 332198), datetime.datetime(2022, 5, 24, 8, 23, 1, 332198), datetime.datetime(2022, 5, 20, 8, 53, 1, 332198)]\n"
     ]
    }
   ],
   "source": [
    "print(posts_posted_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Cleaning our comment count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Viewing our comment counts raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(post_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Iterating over and trimming our comments counts and removing unnecesarry whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_counted_comments = []\n",
    "for cm in post_comments:\n",
    "    cm.split()\n",
    "    if len(cm) > 2:\n",
    "        posts_counted_comments.append(cm[0:2].strip())\n",
    "    else:\n",
    "        posts_counted_comments.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Viewing our cleaned comments counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(posts_counted_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Cleaning our shared counts column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Viewing our raw share count row data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(post_shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cleaning our share count row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_counted_shares = []\n",
    "for ps in post_shared:\n",
    "    ps.split()\n",
    "    if len(ps) > 2:\n",
    "        posts_counted_shares.append(ps[0:2])\n",
    "    else:\n",
    "        posts_counted_shares.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Viewing our cleaned share count column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(posts_counted_shares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Organizing the Data for Export\n",
    "Finally, we are ready to organize our collected data into a Pandas DataFrame and then export it as a CSV or Excel file. You can find this file saved in the same folder that you ran the program from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-a93ae0890533>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         ]\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"our_competitors\": competitors_name,\n",
    "    \"Date_Posted\": posts_posted_date,\n",
    "    \"Media_Type\": media_type,\n",
    "    \"Post_Caption\": post_texts,\n",
    "    \"Likes_Counts\": post_likes,\n",
    "    \"Comments_Counts\": posts_counted_comments,\n",
    "    \"Shared_Counts\" : posts_counted_shares,\n",
    "    \"total_followers\": followers_clean_up\n",
    "#     \"Video Views\": video_views,\n",
    "#     \"Media Links\": media_links\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Saving our scrapped data in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting as csv file to program folder and saving it as linkedin_data\n",
    "# df.to_csv(\"{}.csv\".format(company_name), encoding='utf-8', index=False)\n",
    "df.to_csv(\"{}.csv\".format(\"linkedin_data\"), encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## J. Saving our scrapped data in a Spreadsheet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to Excel file to program folder and saving it as linkedin_data\n",
    "# writer = pd.Excelwriter = pd.ExcelWriter(\"{}.xlsx\".format(company_name), engine='xlsxwriter')\n",
    "writer = pd.Excelwriter = pd.ExcelWriter(\"{}.xlsx\".format(\"linkedin_data\"), engine='xlsxwriter')\n",
    "\n",
    "\n",
    "df.to_excel(writer, index =False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
